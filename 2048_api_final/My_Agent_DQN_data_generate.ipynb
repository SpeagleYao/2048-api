{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded expectmax lib for 2048: /2048-api-master1/game2048/expectimax/bin/2048.so\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 6747752399456974318,\n",
       " name: \"/device:XLA_CPU:0\"\n",
       " device_type: \"XLA_CPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 15990946750850496978\n",
       " physical_device_desc: \"device: XLA_CPU device\",\n",
       " name: \"/device:XLA_GPU:0\"\n",
       " device_type: \"XLA_GPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 9092897201327281547\n",
       " physical_device_desc: \"device: XLA_GPU device\",\n",
       " name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 7390920704\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 17852983205982811388\n",
       " physical_device_desc: \"device: 0, name: Tesla P4, pci bus id: 0000:00:08.0, compute capability: 6.1\"]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output as clear\n",
    "from game2048.game_train import Game_train\n",
    "from game2048.displays import Display, IPythonDisplay\n",
    "import keras\n",
    "import numpy as np\n",
    "import random\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, BatchNormalization, AveragePooling2D, Input, GlobalAveragePooling2D\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from collections import deque\n",
    "from game2048.expectimax import board_to_move\n",
    "#from game2048.expectimax import board_to_move\n",
    "display1 = Display()\n",
    "display2 = IPythonDisplay()\n",
    "tf.test.gpu_device_name()\n",
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_s(state):\n",
    "    state = np.log2(np.maximum(np.array(state), 1)\n",
    "                    ).reshape(1, 4, 4, 1)\n",
    "    state = keras.utils.np_utils.to_categorical(state, 12)\n",
    "    return state\n",
    "\n",
    "def change_back_s(state):\n",
    "    state = np.argmax(state, axis = -1).reshape(4, 4)\n",
    "    state = 2 ** state\n",
    "    state = np.where(state != 1, state, 0)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self):\n",
    "        self.memory = []\n",
    "        self.model_create = 0\n",
    "        self.gamma = 0.9  # the discounted factor of reward\n",
    "        self.epsilon = 0.9 # the ratio of randomly choose\n",
    "        self.epsilon_min = 1e-3  # the min ratio of randomly choose\n",
    "        self.epsilon_decay = 0.999\n",
    "        self.tau = 0.8   #bigger tau will make target nearer\n",
    "        \n",
    "        self.model = self.create_model()\n",
    "        self.target_model = self.create_model()\n",
    "\n",
    "    def create_model(self):\n",
    "        self.model_create += 1\n",
    "\n",
    "        model = Sequential()  # Has 6372 parameters to train\n",
    "        model(Input(shape=(4, 4, 12)))\n",
    "\n",
    "        model.add(Conv2D(8, (4, 4), padding='same', activation='relu'))\n",
    "        model.add(Conv2D(8, (4, 4), padding='same', activation='relu'))\n",
    "        model.add(Flatten())\n",
    "        \n",
    "        model.add(Dense(units=24, activation='relu', kernel_initializer='he_normal'))\n",
    "        model.add(Dense(units=24, activation='relu', kernel_initializer='he_normal'))\n",
    "\n",
    "        model.add(Dense(units=4))\n",
    "\n",
    "        model.compile(loss='mean_squared_error',\n",
    "                      optimizer='adam')\n",
    "\n",
    "        if self.model_create == 1:\n",
    "            model.summary()\n",
    "\n",
    "        return model\n",
    "\n",
    "    def act(self, state):\n",
    "        return board_to_move(change_back_s(state))\n",
    "\n",
    "    def remember(self, state, action, reward, new_state, done):\n",
    "        self.memory.append([state, action, reward, new_state, done])\n",
    "\n",
    "    def replay(self):\n",
    "        batch_size = 256\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "\n",
    "        samples = random.sample(self.memory, batch_size)\n",
    "        states = []\n",
    "        targets = []\n",
    "        for sample in samples:\n",
    "            state, action, reward, new_state, done = sample\n",
    "            target = self.target_model.predict(state)\n",
    "            if done == 0:\n",
    "                Q_future = max(self.target_model.predict(new_state)[0])\n",
    "                target[0][action] = reward + Q_future * self.gamma\n",
    "            elif done == 1:\n",
    "                target[0][action] = reward\n",
    "            elif done == 2:\n",
    "                Q_future = 99999\n",
    "                target[0][action] = reward + Q_future * self.gamma\n",
    "            states.extend(state)\n",
    "            targets.extend(target)\n",
    "        self.model.fit(np.array(states), np.array(targets), batch_size = 16, epochs=20, verbose=0)\n",
    "\n",
    "    def target_train(self):\n",
    "        weights = self.model.get_weights()\n",
    "        target_weights = self.target_model.get_weights()\n",
    "\n",
    "        for i in range(len(target_weights)):\n",
    "            target_weights[i] = weights[i] * self.tau + \\\n",
    "                target_weights[i] * (1 - self.tau)\n",
    "\n",
    "        self.target_model.set_weights(target_weights)\n",
    "\n",
    "    def save_model(self):\n",
    "        self.model.save('my_model_dqn.h5')\n",
    "        \n",
    "    def save_best_model(self):\n",
    "        self.model.save('my_model_best_dqn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 4, 4, 8)           1544      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 4, 4, 8)           1032      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 24)                3096      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 100       \n",
      "=================================================================\n",
      "Total params: 6,372\n",
      "Trainable params: 6,372\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dqn_agent = DQN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_max = []\n",
    "history_total = []\n",
    "history = []\n",
    "total_step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from game2048.agents import Agent\n",
    "import keras\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "\n",
    "class TestAgent(Agent):\n",
    "\n",
    "    def __init__(self, game, display=None):\n",
    "        if game.size != 4:\n",
    "            raise ValueError(\n",
    "                \"`%s` can only work with game of `size` 4.\" % self.__class__.__name__)\n",
    "        super().__init__(game, display)\n",
    "        self.model = load_model('my_model_dqn.h5')\n",
    "\n",
    "    def step(self):\n",
    "        board = np.log2(np.maximum(np.array(self.game.board), 1)).reshape(1, 4, 4, 1)\n",
    "        board = keras.utils.np_utils.to_categorical(board, 12)\n",
    "        direction = self.model.predict_classes(board)[0]\n",
    "        return direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from game2048.game import Game\n",
    "def single_run(size, score_to_win, AgentClass, **kwargs):\n",
    "    game = Game(size, score_to_win)\n",
    "    agent = AgentClass(game, display=Display(), **kwargs)\n",
    "    agent.play(verbose=False)\n",
    "    return game.score\n",
    "\n",
    "N_TESTS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "5\n",
      "10\n",
      "15\n",
      "20\n",
      "25\n",
      "30\n",
      "35\n",
      "40\n",
      "45\n",
      "50\n",
      "55\n",
      "60\n",
      "65\n",
      "70\n",
      "75\n",
      "80\n",
      "85\n",
      "90\n",
      "95\n"
     ]
    }
   ],
   "source": [
    "trials = 100\n",
    "\n",
    "for trial in range(trials):\n",
    "    if trial % 5 == 0: print(trial)\n",
    "    # , score_to_win=2048\n",
    "    game = Game_train(4, score_to_win=128, random=False)\n",
    "    cur_state = change_s(game.board)\n",
    "\n",
    "    while(1):\n",
    "        action = dqn_agent.act(cur_state)\n",
    "        new_state, reward, done = game.move(action)\n",
    "\n",
    "        new_state = change_s(new_state)\n",
    "        dqn_agent.remember(cur_state, action, reward, new_state, done)\n",
    "\n",
    "        #dqn_agent.replay()       # internally iterates default (prediction) model\n",
    "        #dqn_agent.target_train()  # iterates target model\n",
    "\n",
    "        cur_state = new_state\n",
    "\n",
    "        if done:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89092\n"
     ]
    }
   ],
   "source": [
    "print(len(dqn_agent.memory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"data_dqn.npy\", dqn_agent.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
